{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce90184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, if requires_grad=True, the tensor object keeps track of how it was created.\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = torch.tensor([4., 5., 6.], requires_grad=True)\n",
    "#Notice that both x and y have their requires_grad set to true, therefore we can compute gradients with respect ot them\n",
    "z = x + y\n",
    "print(z)\n",
    "#z knows that it was created as a result of addition of x and y. It knows that it wasn't read in from a file\n",
    "print(z.grad_fn)\n",
    "#And if we go further on this\n",
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8807ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now if we backpropagate on s, we can find the gradients of s with respect to x\n",
    "s.backward()\n",
    "print(x.grad)  #ds/dx\n",
    "print(y.grad)  #ds/dy\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5074d2e",
   "metadata": {},
   "source": [
    "print(z.grad) produces a warning. The reason is because by default PyTorch only populate .grad for leaf variables (variables that aren't results of operations), which is x and y in our example. To ensure .grad is also populated for non-leaf variables like z, we need to call their .retain_grad() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de65413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default,tensors have 'requires_grad=False'\n",
    "x = torch.randn(2,2)\n",
    "y = torch.randn(2,2)\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "#so you can't backprop through z\n",
    "print(z.grad_fn)\n",
    "#another way to set the requires_grad=True is \n",
    "x.requires_grad_()\n",
    "y.requires_grad_()\n",
    "# z contains enough information to compute gradients, as we saw above\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "# If any input to an operation has 'requires_grad=True', so will the output\n",
    "print(z.requires_grad)\n",
    "#Now z has the computation history that relates itself to x and y\n",
    "\n",
    "new_z = z.detach()\n",
    "print(new_z.grad_fn)\n",
    "#z.detach() returns a tensor that shares the same storage as 'z', but with the computation history forgotten. \n",
    "# It doesn't know anything about how it was computed. In other words, we have broken the tensor away from its past history\n",
    "\n",
    "#You can also stop autograd from tracking history on tensors. This concept is usedful when applying Transfer Learning\n",
    "#when we want to freeze the network, we don't want to update the weights. we just want to do that for specific layers and this is how we do it\n",
    "print(x.requires_grad)\n",
    "print((x+10).requires_grad)\n",
    "\n",
    "#the following 2 lines are used to stop autograd from tracking\n",
    "with torch.no_grad():\n",
    "    print((x+10).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708be3ff",
   "metadata": {},
   "source": [
    "In PyTorch, the grad_fn attribute of a tensor represents the function that created that tensor. This attribute is None for tensors that were not the result of a computation that requires gradient tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695df8de",
   "metadata": {},
   "source": [
    "Let's understand it in easier way. Now, imagine you have two numbers, one called x and another called y. These numbers can add, subtract, or do other math stuff.\n",
    "\n",
    "When you add x and y to get a new number, let's call it z, it's like saying 2 + 3 = 5. You know the answer is 5, but you don't remember that you added 2 and 3 to get there.\n",
    "\n",
    "In PyTorch, if you set requires_grad=False for x and y, it's like saying, \"I don't care how 2 and 3 made 5; just give me the result 5.\" So, PyTorch doesn't remember how z was created, and z.grad_fn is None.\n",
    "\n",
    "But if you set requires_grad=True for x and y, it's like saying, \"Hey, I want to know not just the result but also how 2 and 3 made 5. Keep track of that for me.\" PyTorch will remember the math (2 + 3) and can tell you how changing 2 or 3 would affect the result, which is useful for things like training machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's walk in through one last example\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print('\\nprinting z and out:')\n",
    "print(z, out)\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587527d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
